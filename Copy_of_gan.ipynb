{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arham786-afg/BMSCE-campus-tour/blob/main/Copy_of_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c25e4f-4905-4a63-8c9a-267cd4bf9ca5",
      "metadata": {
        "id": "69c25e4f-4905-4a63-8c9a-267cd4bf9ca5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from music21 import *\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Dropout, Bidirectional, LSTM\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D,LeakyReLU\n",
        "#from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca144d7-386f-469c-9134-cd5ca303fee7",
      "metadata": {
        "id": "3ca144d7-386f-469c-9134-cd5ca303fee7"
      },
      "outputs": [],
      "source": [
        "def get_notes():\n",
        "  with open('/content/notes_dataset4.pickle', 'rb') as filepath:\n",
        "    notes_array = pickle.load(filepath)\n",
        "    notes_array = list(notes_array)\n",
        "  print(len(notes_array))\n",
        "  return notes_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ed09f8-630d-4260-94a8-c82ea543e0c2",
      "metadata": {
        "id": "e6ed09f8-630d-4260-94a8-c82ea543e0c2"
      },
      "outputs": [],
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    sequence_length = 100\n",
        "\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "    # Create a dictionary to map pitches to integers\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []\n",
        "    network_output = []\n",
        "\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        network_output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # Reshape the input into a format compatible with LSTM layers\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    \n",
        "    # Normalize input between -1 and 1\n",
        "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
        "    network_output = to_categorical(network_output)\n",
        "\n",
        "    return (network_input, network_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13782e0d-994a-4861-a9b8-a2ae832864c6",
      "metadata": {
        "id": "13782e0d-994a-4861-a9b8-a2ae832864c6"
      },
      "outputs": [],
      "source": [
        "# def generate_notes(model, network_input, n_vocab):\n",
        "#     \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "#     # pick a random sequence from the input as a starting point for the prediction\n",
        "#     start = numpy.random.randint(0, len(network_input)-1)\n",
        "    \n",
        "#     # Get pitch names and store in a dictionary\n",
        "#     pitchnames = sorted(set(item for item in notes))\n",
        "#     int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "#     pattern = network_input[start]\n",
        "#     prediction_output = []\n",
        "\n",
        "#     # generate 500 notes\n",
        "#     for note_index in range(500):\n",
        "#         prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "#         prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "#         prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "#         index = numpy.argmax(prediction)\n",
        "#         result = int_to_note[index]\n",
        "#         prediction_output.append(result)\n",
        "        \n",
        "#         pattern = numpy.append(pattern,index)\n",
        "#         #pattern.append(index)\n",
        "#         pattern = pattern[1:len(pattern)]\n",
        "\n",
        "#     return prediction_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457d084e-ec90-4bf4-b2f9-109f93beb312",
      "metadata": {
        "id": "457d084e-ec90-4bf4-b2f9-109f93beb312"
      },
      "outputs": [],
      "source": [
        "def create_midi(prediction_output, filename):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for item in prediction_output:\n",
        "        pattern = item\n",
        "        if ('*' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('*')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        elif ('^' in pattern) and ('/' in pattern):\n",
        "            f = pattern[1:]\n",
        "            f = f.split('/')\n",
        "            ans = float(int(f[0])/int(f[1]))\n",
        "            output_notes.append(note.Rest(quarterLength=ans))\n",
        "        elif ('^' in pattern):\n",
        "            f = float(pattern[1:])\n",
        "            output_notes.append(note.Rest(quarterLength=f))\n",
        "        elif ('/' in pattern):\n",
        "            output_notes.append(meter.TimeSignature(pattern))\n",
        "        elif ('~' in pattern):\n",
        "            metro = pattern.split(\"~\")\n",
        "            output_notes.append(tempo.MetronomeMark(metro[0],float(metro[1])))\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='{}.mid'.format(filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcfc3495-7591-4ad3-b7a1-b111f90cc338",
      "metadata": {
        "id": "bcfc3495-7591-4ad3-b7a1-b111f90cc338"
      },
      "outputs": [],
      "source": [
        "class GAN():\n",
        "    def __init__(self, rows):\n",
        "        self.seq_length = rows\n",
        "        self.seq_shape = (self.seq_length, 1)\n",
        "        self.latent_dim = 1000\n",
        "        self.disc_loss = []\n",
        "        self.gen_loss =[]\n",
        "        \n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates note sequences\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        generated_seq = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(generated_seq)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(512, input_shape=self.seq_shape, return_sequences=True))\n",
        "        model.add(Bidirectional(LSTM(512)))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        seq = Input(shape=self.seq_shape)\n",
        "        validity = model(seq)\n",
        "\n",
        "        return Model(seq, validity)\n",
        "      \n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.seq_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.seq_shape))\n",
        "        model.summary()\n",
        "        \n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        seq = model(noise)\n",
        "\n",
        "        return Model(noise, seq)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load and convert the data\n",
        "        path = os.getcwd()+'/musicnet_midis'\n",
        "\n",
        "        #read all the filenames\n",
        "        #files=[i for i in os.listdir(path) if i.endswith(\".mid\") or i.endswith('midi')]\n",
        "\n",
        "        #reading each midi file\n",
        "        notes = get_notes()\n",
        "        n_vocab = len(set(notes))\n",
        "        X_train, y_train = prepare_sequences(notes, n_vocab)\n",
        "        # Adversarial ground truths\n",
        "        real = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        \n",
        "        # Training the model\n",
        "        for epoch in range(epochs):\n",
        "            # Training the discriminator\n",
        "            # Select a random batch of note sequences\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_seqs = X_train[idx]\n",
        "\n",
        "            #noise = np.random.choice(range(484), (batch_size, self.latent_dim))\n",
        "            #noise = (noise-242)/242\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new note sequences\n",
        "            gen_seqs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(real_seqs, real)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_seqs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "\n",
        "            #  Training the Generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, real)\n",
        "\n",
        "            # Print the progress and save into loss lists\n",
        "            if epoch % sample_interval == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "              self.disc_loss.append(d_loss[0])\n",
        "              self.gen_loss.append(g_loss)\n",
        "        \n",
        "        self.generate(notes)\n",
        "        #self.plot_loss()\n",
        "        \n",
        "    def generate(self, input_notes):\n",
        "        # Get pitch names and store in a dictionary\n",
        "        notes = input_notes\n",
        "        pitchnames = sorted(set(item for item in notes))\n",
        "        mul = len(pitchnames)//2\n",
        "        int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "        \n",
        "        # Use random noise to generate sequences\n",
        "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "        predictions = self.generator.predict(noise)\n",
        "        \n",
        "        pred_notes = [int(x*mul+mul) for x in predictions[0]]\n",
        "        # freq = dict(Counter(pred_notes))\n",
        "        # num = [count for _,count in freq.items()]\n",
        "        # print(num)\n",
        "        # plt.hist(num)\n",
        "        pred_notes = [int_to_note[int(x)] for x in pred_notes]\n",
        "        print(\"after converting:\",pred_notes)\n",
        "        \n",
        "        create_midi(pred_notes, 'gan_final_output')\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        plt.plot(self.disc_loss, c='red')\n",
        "        plt.plot(self.gen_loss, c='blue')\n",
        "        plt.title(\"GAN Loss per Epoch\")\n",
        "        plt.legend(['Discriminator', 'Generator'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.savefig('GAN_Loss_per_Epoch_final.png', transparent=True)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67253a3-aca5-40f0-91e9-c754b27411ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d67253a3-aca5-40f0-91e9-c754b27411ef",
        "outputId": "39e7545a-4173-4110-805f-cc9f65ad60f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100, 512)          1052672   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 1024)             4198400   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,907,457\n",
            "Trainable params: 5,907,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 256)               256256    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               102500    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 100, 1)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,022,820\n",
            "Trainable params: 1,019,236\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "1451908\n",
            "2/2 [==============================] - 2s 4ms/step\n",
            "0 [D loss: 0.695812, acc.: 15.62%] [G loss: 0.692417]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "1 [D loss: 0.665619, acc.: 79.69%] [G loss: 0.692730]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "2 [D loss: 0.626692, acc.: 78.12%] [G loss: 0.690623]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "3 [D loss: 0.540918, acc.: 79.69%] [G loss: 0.731661]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "4 [D loss: 0.362502, acc.: 82.03%] [G loss: 0.893089]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "5 [D loss: 0.256810, acc.: 91.41%] [G loss: 1.465662]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "6 [D loss: 0.108884, acc.: 97.66%] [G loss: 6.213963]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "7 [D loss: 0.040232, acc.: 98.44%] [G loss: 14.976056]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "8 [D loss: 0.091250, acc.: 97.66%] [G loss: 7.765773]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "9 [D loss: 0.082725, acc.: 96.09%] [G loss: 4.270011]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "10 [D loss: 0.106955, acc.: 98.44%] [G loss: 7.860673]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "11 [D loss: 0.111575, acc.: 97.66%] [G loss: 5.963593]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "12 [D loss: 0.056426, acc.: 96.88%] [G loss: 5.321468]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "13 [D loss: 0.069011, acc.: 97.66%] [G loss: 6.895052]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "14 [D loss: 0.284081, acc.: 93.75%] [G loss: 5.544009]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "15 [D loss: 0.056743, acc.: 99.22%] [G loss: 6.202826]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "16 [D loss: 0.095644, acc.: 97.66%] [G loss: 11.582939]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "17 [D loss: 0.066727, acc.: 97.66%] [G loss: 14.641174]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "18 [D loss: 0.267579, acc.: 89.84%] [G loss: 5.598208]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "19 [D loss: 0.395803, acc.: 86.72%] [G loss: 2.926192]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "20 [D loss: 0.190192, acc.: 91.41%] [G loss: 3.615168]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "21 [D loss: 0.205874, acc.: 89.84%] [G loss: 3.778208]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "22 [D loss: 0.166200, acc.: 90.62%] [G loss: 4.147356]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "23 [D loss: 0.153300, acc.: 94.53%] [G loss: 5.073461]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "24 [D loss: 0.118974, acc.: 96.09%] [G loss: 6.571416]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "25 [D loss: 0.100760, acc.: 95.31%] [G loss: 7.640050]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "26 [D loss: 0.152280, acc.: 93.75%] [G loss: 8.109403]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "27 [D loss: 0.095911, acc.: 97.66%] [G loss: 8.539791]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "28 [D loss: 0.159916, acc.: 95.31%] [G loss: 9.827565]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "29 [D loss: 0.061193, acc.: 98.44%] [G loss: 11.939028]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "30 [D loss: 0.071383, acc.: 97.66%] [G loss: 14.752376]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "31 [D loss: 0.037749, acc.: 99.22%] [G loss: 20.165525]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "32 [D loss: 0.218558, acc.: 94.53%] [G loss: 11.654926]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "33 [D loss: 0.223778, acc.: 94.53%] [G loss: 5.791732]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "34 [D loss: 0.212196, acc.: 92.19%] [G loss: 7.127597]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "35 [D loss: 0.153782, acc.: 96.09%] [G loss: 8.390682]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "36 [D loss: 0.181988, acc.: 96.09%] [G loss: 7.660914]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "37 [D loss: 0.126546, acc.: 97.66%] [G loss: 8.391741]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "38 [D loss: 0.257540, acc.: 90.62%] [G loss: 7.680675]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "39 [D loss: 0.159758, acc.: 96.88%] [G loss: 8.714745]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "40 [D loss: 0.154167, acc.: 96.88%] [G loss: 12.140965]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "41 [D loss: 0.124112, acc.: 96.09%] [G loss: 13.366639]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "42 [D loss: 0.321362, acc.: 89.84%] [G loss: 4.714757]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "43 [D loss: 0.199782, acc.: 94.53%] [G loss: 5.349751]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "44 [D loss: 0.190326, acc.: 94.53%] [G loss: 9.179205]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "45 [D loss: 0.124582, acc.: 94.53%] [G loss: 13.507395]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "46 [D loss: 0.158176, acc.: 96.09%] [G loss: 16.717636]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "47 [D loss: 0.672400, acc.: 85.16%] [G loss: 2.998248]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "48 [D loss: 0.303296, acc.: 89.06%] [G loss: 2.801704]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "49 [D loss: 0.302855, acc.: 92.19%] [G loss: 4.280358]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "50 [D loss: 0.270788, acc.: 91.41%] [G loss: 6.301520]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "51 [D loss: 0.204347, acc.: 94.53%] [G loss: 6.722503]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "52 [D loss: 0.308994, acc.: 89.84%] [G loss: 3.778283]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "53 [D loss: 0.167126, acc.: 97.66%] [G loss: 4.173677]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "54 [D loss: 0.343741, acc.: 88.28%] [G loss: 5.170101]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "55 [D loss: 0.304064, acc.: 90.62%] [G loss: 5.846317]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "56 [D loss: 0.257825, acc.: 93.75%] [G loss: 5.323835]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "57 [D loss: 0.261436, acc.: 92.19%] [G loss: 8.028613]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "58 [D loss: 0.314502, acc.: 91.41%] [G loss: 6.372618]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "59 [D loss: 0.302947, acc.: 92.19%] [G loss: 5.887070]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "60 [D loss: 0.346062, acc.: 91.41%] [G loss: 3.806455]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "61 [D loss: 0.356843, acc.: 90.62%] [G loss: 4.156694]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "62 [D loss: 0.294433, acc.: 92.97%] [G loss: 4.691406]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "63 [D loss: 0.279361, acc.: 92.19%] [G loss: 5.612513]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "64 [D loss: 0.238597, acc.: 92.19%] [G loss: 7.430507]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "65 [D loss: 0.191771, acc.: 92.97%] [G loss: 10.886782]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "66 [D loss: 0.595664, acc.: 84.38%] [G loss: 5.777449]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "67 [D loss: 2.137463, acc.: 40.62%] [G loss: 0.869632]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "68 [D loss: 0.567982, acc.: 87.50%] [G loss: 0.806451]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "69 [D loss: 0.579321, acc.: 84.38%] [G loss: 0.820985]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "70 [D loss: 0.552446, acc.: 79.69%] [G loss: 0.877670]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "71 [D loss: 0.479790, acc.: 85.16%] [G loss: 0.997067]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "72 [D loss: 0.449540, acc.: 87.50%] [G loss: 1.280545]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "73 [D loss: 0.559169, acc.: 80.47%] [G loss: 1.758306]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "74 [D loss: 0.432519, acc.: 82.81%] [G loss: 3.340632]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "75 [D loss: 0.404251, acc.: 85.94%] [G loss: 8.078613]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "76 [D loss: 0.528233, acc.: 88.28%] [G loss: 4.011319]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "77 [D loss: 0.502800, acc.: 82.03%] [G loss: 1.684717]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "78 [D loss: 0.447531, acc.: 85.94%] [G loss: 2.064014]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "79 [D loss: 0.527565, acc.: 77.34%] [G loss: 2.461110]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "80 [D loss: 0.467581, acc.: 79.69%] [G loss: 3.445828]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "81 [D loss: 0.415034, acc.: 83.59%] [G loss: 3.843103]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "82 [D loss: 0.455364, acc.: 81.25%] [G loss: 3.189977]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "83 [D loss: 0.377688, acc.: 88.28%] [G loss: 3.261116]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "84 [D loss: 0.490399, acc.: 82.81%] [G loss: 3.749753]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "85 [D loss: 0.436945, acc.: 84.38%] [G loss: 4.327271]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "86 [D loss: 0.480825, acc.: 79.69%] [G loss: 4.241322]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "87 [D loss: 0.398183, acc.: 84.38%] [G loss: 4.901117]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "88 [D loss: 0.388171, acc.: 85.94%] [G loss: 5.615379]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "89 [D loss: 0.415905, acc.: 86.72%] [G loss: 3.637674]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "90 [D loss: 0.485543, acc.: 79.69%] [G loss: 2.881898]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "91 [D loss: 0.405169, acc.: 85.94%] [G loss: 2.909838]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "92 [D loss: 0.394557, acc.: 85.16%] [G loss: 3.581919]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "93 [D loss: 0.388013, acc.: 85.94%] [G loss: 4.149139]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "94 [D loss: 0.477778, acc.: 81.25%] [G loss: 3.213102]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "95 [D loss: 0.426241, acc.: 82.03%] [G loss: 3.185376]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "96 [D loss: 0.339020, acc.: 90.62%] [G loss: 3.073510]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "97 [D loss: 0.389081, acc.: 86.72%] [G loss: 3.036336]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "98 [D loss: 0.339406, acc.: 89.84%] [G loss: 3.787359]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "99 [D loss: 0.303011, acc.: 92.19%] [G loss: 4.377539]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "100 [D loss: 0.434921, acc.: 81.25%] [G loss: 4.374103]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "101 [D loss: 0.348107, acc.: 87.50%] [G loss: 3.681935]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "102 [D loss: 0.328095, acc.: 88.28%] [G loss: 4.293851]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "103 [D loss: 0.391315, acc.: 85.94%] [G loss: 3.702809]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "104 [D loss: 0.373336, acc.: 87.50%] [G loss: 4.111057]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "105 [D loss: 0.363199, acc.: 89.06%] [G loss: 3.919081]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "106 [D loss: 0.439503, acc.: 80.47%] [G loss: 2.988696]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "107 [D loss: 0.333594, acc.: 89.84%] [G loss: 3.888113]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "108 [D loss: 0.429208, acc.: 83.59%] [G loss: 3.082049]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "109 [D loss: 0.321129, acc.: 92.19%] [G loss: 2.904469]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "110 [D loss: 0.387746, acc.: 86.72%] [G loss: 4.243909]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "111 [D loss: 0.350298, acc.: 90.62%] [G loss: 4.995675]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "112 [D loss: 0.296429, acc.: 92.19%] [G loss: 4.683455]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "113 [D loss: 0.348865, acc.: 85.94%] [G loss: 6.299963]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "114 [D loss: 0.299797, acc.: 89.84%] [G loss: 7.065091]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "115 [D loss: 0.346020, acc.: 92.19%] [G loss: 6.762201]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "116 [D loss: 0.384396, acc.: 90.62%] [G loss: 5.456172]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "117 [D loss: 2.193812, acc.: 39.84%] [G loss: 3.599096]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "118 [D loss: 1.234147, acc.: 32.03%] [G loss: 0.811817]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "119 [D loss: 0.608940, acc.: 75.00%] [G loss: 0.809584]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "120 [D loss: 0.623108, acc.: 71.09%] [G loss: 0.800768]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "121 [D loss: 0.545170, acc.: 78.12%] [G loss: 0.901108]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "122 [D loss: 0.627669, acc.: 72.66%] [G loss: 0.949982]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "123 [D loss: 0.567748, acc.: 75.78%] [G loss: 0.996733]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "124 [D loss: 0.512327, acc.: 82.03%] [G loss: 1.079516]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "125 [D loss: 0.608707, acc.: 73.44%] [G loss: 1.109782]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "126 [D loss: 0.531773, acc.: 76.56%] [G loss: 1.215853]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "127 [D loss: 0.555756, acc.: 75.78%] [G loss: 1.213531]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "128 [D loss: 0.555345, acc.: 78.12%] [G loss: 1.390002]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "129 [D loss: 0.492286, acc.: 82.03%] [G loss: 1.405184]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "130 [D loss: 0.506376, acc.: 77.34%] [G loss: 1.618471]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "131 [D loss: 0.549238, acc.: 75.00%] [G loss: 1.691012]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "132 [D loss: 0.505823, acc.: 76.56%] [G loss: 2.225378]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "133 [D loss: 0.441277, acc.: 85.94%] [G loss: 2.797893]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "134 [D loss: 0.448976, acc.: 83.59%] [G loss: 2.934443]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "135 [D loss: 0.492013, acc.: 78.91%] [G loss: 3.896973]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "136 [D loss: 0.441514, acc.: 83.59%] [G loss: 2.936831]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "137 [D loss: 0.398826, acc.: 88.28%] [G loss: 2.434122]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "138 [D loss: 0.400745, acc.: 85.94%] [G loss: 3.117713]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "139 [D loss: 0.394747, acc.: 84.38%] [G loss: 3.877818]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "140 [D loss: 0.371048, acc.: 85.94%] [G loss: 4.444542]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "141 [D loss: 0.447205, acc.: 83.59%] [G loss: 3.556550]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "142 [D loss: 0.484736, acc.: 78.12%] [G loss: 4.081180]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "143 [D loss: 0.429748, acc.: 80.47%] [G loss: 4.297216]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "144 [D loss: 0.493537, acc.: 81.25%] [G loss: 3.600917]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "145 [D loss: 0.496603, acc.: 83.59%] [G loss: 3.182244]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "146 [D loss: 0.605995, acc.: 69.53%] [G loss: 2.890847]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "147 [D loss: 0.422885, acc.: 80.47%] [G loss: 3.911244]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "148 [D loss: 0.549813, acc.: 73.44%] [G loss: 2.042096]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "149 [D loss: 0.745367, acc.: 42.97%] [G loss: 1.678596]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "150 [D loss: 0.691874, acc.: 32.03%] [G loss: 1.545066]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "151 [D loss: 0.589016, acc.: 75.78%] [G loss: 1.794842]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "152 [D loss: 0.536536, acc.: 74.22%] [G loss: 1.967351]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "153 [D loss: 0.532018, acc.: 76.56%] [G loss: 2.548908]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "154 [D loss: 0.502991, acc.: 76.56%] [G loss: 2.696645]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "155 [D loss: 0.456474, acc.: 80.47%] [G loss: 3.199447]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "156 [D loss: 0.484301, acc.: 76.56%] [G loss: 3.194669]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "157 [D loss: 0.453103, acc.: 82.03%] [G loss: 2.959765]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "158 [D loss: 0.528532, acc.: 78.12%] [G loss: 2.202023]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "159 [D loss: 0.445152, acc.: 82.03%] [G loss: 2.304536]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "160 [D loss: 0.432168, acc.: 82.03%] [G loss: 2.756810]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "161 [D loss: 0.404832, acc.: 87.50%] [G loss: 2.703860]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "162 [D loss: 0.378954, acc.: 85.94%] [G loss: 3.599066]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "163 [D loss: 0.423481, acc.: 80.47%] [G loss: 3.285341]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "164 [D loss: 0.350157, acc.: 86.72%] [G loss: 3.483178]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "165 [D loss: 0.509577, acc.: 76.56%] [G loss: 3.340571]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "166 [D loss: 0.370861, acc.: 84.38%] [G loss: 4.056071]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "167 [D loss: 0.373810, acc.: 82.81%] [G loss: 4.594638]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "168 [D loss: 0.543368, acc.: 79.69%] [G loss: 2.951614]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "169 [D loss: 0.515814, acc.: 78.12%] [G loss: 2.466034]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "170 [D loss: 0.422942, acc.: 79.69%] [G loss: 2.394834]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "171 [D loss: 0.592776, acc.: 76.56%] [G loss: 2.474322]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "172 [D loss: 0.470116, acc.: 78.12%] [G loss: 2.699476]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "173 [D loss: 0.449137, acc.: 80.47%] [G loss: 2.590596]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "174 [D loss: 0.422673, acc.: 80.47%] [G loss: 2.827133]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "175 [D loss: 0.433222, acc.: 82.03%] [G loss: 2.468853]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "176 [D loss: 0.364806, acc.: 87.50%] [G loss: 3.179399]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "177 [D loss: 0.440226, acc.: 80.47%] [G loss: 3.280763]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "178 [D loss: 0.411910, acc.: 82.03%] [G loss: 3.322489]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "179 [D loss: 0.458008, acc.: 78.12%] [G loss: 2.707833]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "180 [D loss: 0.419965, acc.: 80.47%] [G loss: 2.977359]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "181 [D loss: 0.439600, acc.: 82.03%] [G loss: 3.121215]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "182 [D loss: 0.400062, acc.: 86.72%] [G loss: 3.042327]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "183 [D loss: 0.336546, acc.: 86.72%] [G loss: 3.742679]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "184 [D loss: 0.424045, acc.: 91.41%] [G loss: 3.121165]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "185 [D loss: 0.404882, acc.: 88.28%] [G loss: 2.907465]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "186 [D loss: 0.344205, acc.: 91.41%] [G loss: 3.123359]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "187 [D loss: 0.918805, acc.: 57.03%] [G loss: 2.719859]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "188 [D loss: 1.148478, acc.: 32.81%] [G loss: 1.392239]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "189 [D loss: 0.666544, acc.: 49.22%] [G loss: 1.257113]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "190 [D loss: 0.670043, acc.: 68.75%] [G loss: 1.229013]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "191 [D loss: 0.532524, acc.: 78.91%] [G loss: 1.368163]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "192 [D loss: 0.512339, acc.: 79.69%] [G loss: 1.479879]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "193 [D loss: 0.533101, acc.: 77.34%] [G loss: 1.377635]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "194 [D loss: 0.523979, acc.: 75.78%] [G loss: 1.522790]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "195 [D loss: 0.489448, acc.: 82.81%] [G loss: 1.497736]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "196 [D loss: 0.421030, acc.: 86.72%] [G loss: 1.613287]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "197 [D loss: 0.505715, acc.: 76.56%] [G loss: 1.603948]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "198 [D loss: 0.436604, acc.: 82.03%] [G loss: 1.762113]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "199 [D loss: 0.458937, acc.: 78.91%] [G loss: 1.803198]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "200 [D loss: 0.462680, acc.: 82.03%] [G loss: 1.957915]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "201 [D loss: 0.432737, acc.: 80.47%] [G loss: 1.833933]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "202 [D loss: 0.475872, acc.: 79.69%] [G loss: 2.092546]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "203 [D loss: 0.446688, acc.: 78.91%] [G loss: 2.317987]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "204 [D loss: 0.398310, acc.: 85.16%] [G loss: 2.766229]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "205 [D loss: 0.423589, acc.: 82.81%] [G loss: 2.664900]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "206 [D loss: 0.338654, acc.: 84.38%] [G loss: 3.483667]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "207 [D loss: 0.392332, acc.: 85.16%] [G loss: 2.992882]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "208 [D loss: 0.966436, acc.: 42.97%] [G loss: 1.348560]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "209 [D loss: 0.684418, acc.: 61.72%] [G loss: 1.568144]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "210 [D loss: 0.528162, acc.: 78.12%] [G loss: 1.476981]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "211 [D loss: 0.445559, acc.: 82.03%] [G loss: 2.096109]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "212 [D loss: 0.730619, acc.: 56.25%] [G loss: 1.607072]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "213 [D loss: 0.423726, acc.: 87.50%] [G loss: 2.037959]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "214 [D loss: 0.618298, acc.: 84.38%] [G loss: 8.394826]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "215 [D loss: 4.954242, acc.: 50.00%] [G loss: 1.683506]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "216 [D loss: 0.601558, acc.: 50.78%] [G loss: 1.545592]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "217 [D loss: 0.537973, acc.: 73.44%] [G loss: 1.486509]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "218 [D loss: 0.494823, acc.: 85.16%] [G loss: 1.485193]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "219 [D loss: 0.518857, acc.: 80.47%] [G loss: 1.496621]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "220 [D loss: 0.471049, acc.: 81.25%] [G loss: 1.491636]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "221 [D loss: 0.486767, acc.: 88.28%] [G loss: 1.341854]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "222 [D loss: 0.506739, acc.: 79.69%] [G loss: 1.995964]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "223 [D loss: 0.384585, acc.: 89.84%] [G loss: 1.955217]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "224 [D loss: 0.480082, acc.: 79.69%] [G loss: 1.636481]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "225 [D loss: 0.513564, acc.: 74.22%] [G loss: 1.395525]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "226 [D loss: 0.479332, acc.: 78.91%] [G loss: 1.416437]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "227 [D loss: 0.439154, acc.: 80.47%] [G loss: 1.417319]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "228 [D loss: 0.434357, acc.: 79.69%] [G loss: 1.472682]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "229 [D loss: 0.348718, acc.: 86.72%] [G loss: 1.706510]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "230 [D loss: 0.376179, acc.: 86.72%] [G loss: 1.812167]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "231 [D loss: 0.348179, acc.: 86.72%] [G loss: 1.915161]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "232 [D loss: 0.386374, acc.: 83.59%] [G loss: 2.498891]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "233 [D loss: 0.356980, acc.: 85.16%] [G loss: 3.256143]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "234 [D loss: 0.281830, acc.: 89.84%] [G loss: 4.208537]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "235 [D loss: 0.320258, acc.: 87.50%] [G loss: 3.941758]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "236 [D loss: 0.257365, acc.: 92.19%] [G loss: 5.089652]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "237 [D loss: 0.314215, acc.: 86.72%] [G loss: 5.741855]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "238 [D loss: 0.218942, acc.: 94.53%] [G loss: 8.844007]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "239 [D loss: 1.827500, acc.: 47.66%] [G loss: 1.435882]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "240 [D loss: 0.470331, acc.: 73.44%] [G loss: 1.294950]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "241 [D loss: 0.423743, acc.: 81.25%] [G loss: 1.401436]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "242 [D loss: 0.412220, acc.: 90.62%] [G loss: 1.377288]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "243 [D loss: 0.402463, acc.: 89.06%] [G loss: 1.325030]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "244 [D loss: 0.415692, acc.: 85.94%] [G loss: 1.462354]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "245 [D loss: 0.440762, acc.: 82.03%] [G loss: 1.507638]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "246 [D loss: 0.417521, acc.: 87.50%] [G loss: 1.541251]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "247 [D loss: 0.418645, acc.: 83.59%] [G loss: 1.506956]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "248 [D loss: 0.357683, acc.: 91.41%] [G loss: 1.570985]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "249 [D loss: 0.315051, acc.: 92.97%] [G loss: 1.690224]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "250 [D loss: 0.318824, acc.: 89.84%] [G loss: 1.731778]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "251 [D loss: 0.336345, acc.: 89.06%] [G loss: 1.845183]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "252 [D loss: 0.326367, acc.: 85.94%] [G loss: 2.173604]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "253 [D loss: 0.349268, acc.: 83.59%] [G loss: 2.119782]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "254 [D loss: 0.356043, acc.: 85.94%] [G loss: 2.212692]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "255 [D loss: 0.345678, acc.: 85.16%] [G loss: 2.410337]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "256 [D loss: 0.277824, acc.: 95.31%] [G loss: 2.866167]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "257 [D loss: 0.287257, acc.: 91.41%] [G loss: 2.947945]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "258 [D loss: 0.131562, acc.: 100.00%] [G loss: 3.915315]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "259 [D loss: 0.244955, acc.: 89.06%] [G loss: 4.380059]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "260 [D loss: 0.282178, acc.: 89.84%] [G loss: 3.458209]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "261 [D loss: 0.147143, acc.: 96.88%] [G loss: 3.905182]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "262 [D loss: 0.022187, acc.: 99.22%] [G loss: 4.494232]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "263 [D loss: 0.116359, acc.: 94.53%] [G loss: 4.394987]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "264 [D loss: 1.583472, acc.: 42.19%] [G loss: 1.325680]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "265 [D loss: 0.417022, acc.: 80.47%] [G loss: 1.398373]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "266 [D loss: 0.363828, acc.: 90.62%] [G loss: 1.386968]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "267 [D loss: 0.319309, acc.: 91.41%] [G loss: 1.377106]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "268 [D loss: 0.296806, acc.: 92.97%] [G loss: 1.544320]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "269 [D loss: 0.297954, acc.: 89.84%] [G loss: 1.835870]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "270 [D loss: 0.264268, acc.: 95.31%] [G loss: 2.526965]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "271 [D loss: 0.270741, acc.: 92.19%] [G loss: 2.842464]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "272 [D loss: 0.118952, acc.: 97.66%] [G loss: 3.998986]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "273 [D loss: 0.172406, acc.: 93.75%] [G loss: 3.390494]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "274 [D loss: 0.098919, acc.: 95.31%] [G loss: 4.024068]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "275 [D loss: 0.064299, acc.: 97.66%] [G loss: 4.677313]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "276 [D loss: 0.173661, acc.: 96.09%] [G loss: 4.531240]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "277 [D loss: 0.068940, acc.: 96.88%] [G loss: 4.931297]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "278 [D loss: 0.076142, acc.: 97.66%] [G loss: 5.165370]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "279 [D loss: 0.060188, acc.: 98.44%] [G loss: 5.858166]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "280 [D loss: 0.097050, acc.: 96.09%] [G loss: 5.501229]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "281 [D loss: 0.232656, acc.: 90.62%] [G loss: 4.544300]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "282 [D loss: 0.237807, acc.: 92.97%] [G loss: 4.538144]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "283 [D loss: 0.112075, acc.: 97.66%] [G loss: 4.629801]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "284 [D loss: 0.052888, acc.: 98.44%] [G loss: 6.702510]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "285 [D loss: 0.103479, acc.: 97.66%] [G loss: 6.781599]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "286 [D loss: 0.048021, acc.: 97.66%] [G loss: 6.644407]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "287 [D loss: 0.100192, acc.: 98.44%] [G loss: 6.132557]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "288 [D loss: 0.157415, acc.: 95.31%] [G loss: 7.979694]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "289 [D loss: 0.219879, acc.: 92.97%] [G loss: 4.874824]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "290 [D loss: 0.315793, acc.: 96.88%] [G loss: 2.764781]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "291 [D loss: 0.180930, acc.: 99.22%] [G loss: 2.724118]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "292 [D loss: 0.144226, acc.: 96.09%] [G loss: 3.606417]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "293 [D loss: 0.116461, acc.: 96.88%] [G loss: 4.205760]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "294 [D loss: 0.074947, acc.: 97.66%] [G loss: 5.140182]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "295 [D loss: 0.055989, acc.: 99.22%] [G loss: 5.318822]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "296 [D loss: 0.125517, acc.: 97.66%] [G loss: 4.733329]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "297 [D loss: 0.110515, acc.: 96.09%] [G loss: 4.279428]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "298 [D loss: 0.135985, acc.: 97.66%] [G loss: 4.137908]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "299 [D loss: 0.058051, acc.: 98.44%] [G loss: 4.535385]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "300 [D loss: 0.089049, acc.: 97.66%] [G loss: 4.126038]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "301 [D loss: 0.330362, acc.: 88.28%] [G loss: 2.777833]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "302 [D loss: 0.183786, acc.: 97.66%] [G loss: 2.561762]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "303 [D loss: 0.170246, acc.: 96.88%] [G loss: 3.291706]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "304 [D loss: 0.163830, acc.: 95.31%] [G loss: 4.097498]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "305 [D loss: 0.075788, acc.: 96.88%] [G loss: 4.869267]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "306 [D loss: 0.205338, acc.: 92.97%] [G loss: 3.663146]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "307 [D loss: 0.172346, acc.: 93.75%] [G loss: 3.829010]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "308 [D loss: 0.191194, acc.: 92.19%] [G loss: 3.340885]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "309 [D loss: 0.102436, acc.: 96.88%] [G loss: 4.225193]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "310 [D loss: 0.087671, acc.: 97.66%] [G loss: 4.369266]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "311 [D loss: 0.129171, acc.: 94.53%] [G loss: 3.673038]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "312 [D loss: 0.281838, acc.: 90.62%] [G loss: 2.734095]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "313 [D loss: 0.146576, acc.: 96.88%] [G loss: 3.483880]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "314 [D loss: 0.078944, acc.: 96.88%] [G loss: 4.073902]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "315 [D loss: 0.218353, acc.: 91.41%] [G loss: 3.225746]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "316 [D loss: 0.182591, acc.: 95.31%] [G loss: 3.563088]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "317 [D loss: 0.104945, acc.: 96.88%] [G loss: 4.038613]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "318 [D loss: 0.281499, acc.: 91.41%] [G loss: 3.373194]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "319 [D loss: 0.214879, acc.: 89.06%] [G loss: 3.746078]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "320 [D loss: 0.057203, acc.: 98.44%] [G loss: 4.587791]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "321 [D loss: 0.185658, acc.: 92.97%] [G loss: 3.627430]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "322 [D loss: 0.091666, acc.: 96.88%] [G loss: 3.532088]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "323 [D loss: 0.104194, acc.: 95.31%] [G loss: 3.655839]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "324 [D loss: 0.057449, acc.: 98.44%] [G loss: 4.328748]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "325 [D loss: 0.048621, acc.: 98.44%] [G loss: 4.469102]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "326 [D loss: 0.137073, acc.: 97.66%] [G loss: 4.015564]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "327 [D loss: 0.134810, acc.: 96.09%] [G loss: 3.554487]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "328 [D loss: 0.184685, acc.: 94.53%] [G loss: 3.064009]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "329 [D loss: 0.114470, acc.: 97.66%] [G loss: 3.627001]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "330 [D loss: 0.060312, acc.: 97.66%] [G loss: 4.536183]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "331 [D loss: 0.385228, acc.: 89.84%] [G loss: 3.030128]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "332 [D loss: 0.652030, acc.: 53.12%] [G loss: 1.990157]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "333 [D loss: 0.417089, acc.: 85.16%] [G loss: 1.876237]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "334 [D loss: 0.284784, acc.: 91.41%] [G loss: 1.898767]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "335 [D loss: 0.333705, acc.: 88.28%] [G loss: 1.886605]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "336 [D loss: 0.344838, acc.: 89.06%] [G loss: 2.258701]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "337 [D loss: 0.266904, acc.: 91.41%] [G loss: 2.152592]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "338 [D loss: 0.250061, acc.: 93.75%] [G loss: 2.298770]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "339 [D loss: 0.298146, acc.: 92.19%] [G loss: 2.416897]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "340 [D loss: 0.197480, acc.: 92.97%] [G loss: 2.485726]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "341 [D loss: 0.191340, acc.: 96.88%] [G loss: 2.644949]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "342 [D loss: 0.170110, acc.: 94.53%] [G loss: 2.952414]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "343 [D loss: 0.209395, acc.: 93.75%] [G loss: 3.367376]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "344 [D loss: 0.197947, acc.: 94.53%] [G loss: 3.907329]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "345 [D loss: 0.142783, acc.: 96.88%] [G loss: 3.540082]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "346 [D loss: 0.102589, acc.: 98.44%] [G loss: 3.951569]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "347 [D loss: 0.176984, acc.: 93.75%] [G loss: 4.141775]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "348 [D loss: 0.259669, acc.: 90.62%] [G loss: 4.081600]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "349 [D loss: 0.157727, acc.: 96.09%] [G loss: 3.872818]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "350 [D loss: 0.289872, acc.: 91.41%] [G loss: 3.873482]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "351 [D loss: 0.126840, acc.: 96.09%] [G loss: 3.591528]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "352 [D loss: 0.281610, acc.: 89.06%] [G loss: 3.845169]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "353 [D loss: 0.311495, acc.: 92.97%] [G loss: 3.070207]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "354 [D loss: 0.318104, acc.: 89.06%] [G loss: 3.321883]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "355 [D loss: 0.148537, acc.: 95.31%] [G loss: 3.111271]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "356 [D loss: 0.309468, acc.: 86.72%] [G loss: 3.369030]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "357 [D loss: 0.292460, acc.: 87.50%] [G loss: 2.659170]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "358 [D loss: 0.201141, acc.: 91.41%] [G loss: 2.547105]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "359 [D loss: 0.113496, acc.: 98.44%] [G loss: 2.980428]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "360 [D loss: 0.183674, acc.: 96.88%] [G loss: 3.275226]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "361 [D loss: 0.179679, acc.: 95.31%] [G loss: 3.147179]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "362 [D loss: 0.189822, acc.: 95.31%] [G loss: 3.100398]\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "363 [D loss: 0.067285, acc.: 98.44%] [G loss: 3.601910]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "364 [D loss: 0.109301, acc.: 96.09%] [G loss: 3.681736]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "365 [D loss: 0.091109, acc.: 96.88%] [G loss: 3.863398]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "366 [D loss: 0.352924, acc.: 87.50%] [G loss: 3.523207]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "367 [D loss: 0.840286, acc.: 50.00%] [G loss: 1.686736]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "368 [D loss: 0.290367, acc.: 87.50%] [G loss: 1.712374]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "369 [D loss: 0.274453, acc.: 91.41%] [G loss: 1.832860]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "370 [D loss: 0.269667, acc.: 92.19%] [G loss: 2.003645]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "371 [D loss: 0.257967, acc.: 89.84%] [G loss: 1.970296]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "372 [D loss: 0.164183, acc.: 96.09%] [G loss: 2.301997]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "373 [D loss: 0.243299, acc.: 93.75%] [G loss: 2.323933]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "374 [D loss: 0.146187, acc.: 96.88%] [G loss: 2.713037]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "375 [D loss: 0.180663, acc.: 94.53%] [G loss: 2.747476]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "376 [D loss: 0.709501, acc.: 87.50%] [G loss: 2.690438]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "377 [D loss: 0.861710, acc.: 50.00%] [G loss: 2.332884]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "378 [D loss: 0.822584, acc.: 50.00%] [G loss: 1.819300]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "379 [D loss: 0.611999, acc.: 50.00%] [G loss: 1.575493]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "380 [D loss: 0.592898, acc.: 50.00%] [G loss: 1.523542]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "381 [D loss: 0.600019, acc.: 50.00%] [G loss: 1.533555]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "382 [D loss: 0.541423, acc.: 68.75%] [G loss: 1.505989]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "383 [D loss: 0.504789, acc.: 77.34%] [G loss: 1.493727]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "384 [D loss: 0.482851, acc.: 85.16%] [G loss: 1.348341]\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "385 [D loss: 0.322616, acc.: 89.06%] [G loss: 1.397878]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "386 [D loss: 0.395588, acc.: 82.81%] [G loss: 1.792180]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "387 [D loss: 0.240914, acc.: 96.88%] [G loss: 1.976342]\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "388 [D loss: 0.291567, acc.: 86.72%] [G loss: 2.460949]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "389 [D loss: 0.210155, acc.: 91.41%] [G loss: 2.792110]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "390 [D loss: 0.188838, acc.: 95.31%] [G loss: 2.920596]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "391 [D loss: 0.195223, acc.: 93.75%] [G loss: 2.922701]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "392 [D loss: 0.225438, acc.: 95.31%] [G loss: 2.987445]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "393 [D loss: 0.289872, acc.: 89.06%] [G loss: 2.623174]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "394 [D loss: 0.602781, acc.: 50.00%] [G loss: 2.217972]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "395 [D loss: 0.575841, acc.: 50.00%] [G loss: 1.951903]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "396 [D loss: 0.540072, acc.: 67.97%] [G loss: 1.702736]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "397 [D loss: 0.522311, acc.: 75.00%] [G loss: 1.686121]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "398 [D loss: 0.466998, acc.: 82.03%] [G loss: 1.554658]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "399 [D loss: 0.446110, acc.: 85.16%] [G loss: 1.788185]\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "400 [D loss: 0.419320, acc.: 80.47%] [G loss: 1.771779]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "401 [D loss: 0.373959, acc.: 89.06%] [G loss: 1.814551]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "402 [D loss: 0.321141, acc.: 92.97%] [G loss: 1.827750]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "403 [D loss: 0.280854, acc.: 92.97%] [G loss: 2.205702]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "404 [D loss: 0.371357, acc.: 89.06%] [G loss: 2.554147]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "405 [D loss: 0.294573, acc.: 91.41%] [G loss: 2.653506]\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "406 [D loss: 0.311337, acc.: 90.62%] [G loss: 2.556105]\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "407 [D loss: 0.270525, acc.: 89.84%] [G loss: 3.072955]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "408 [D loss: 0.192020, acc.: 94.53%] [G loss: 3.255592]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "409 [D loss: 0.180369, acc.: 93.75%] [G loss: 4.680351]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "410 [D loss: 0.230349, acc.: 91.41%] [G loss: 3.317421]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "411 [D loss: 0.265641, acc.: 90.62%] [G loss: 4.434953]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "412 [D loss: 0.334498, acc.: 90.62%] [G loss: 2.836671]\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "413 [D loss: 0.194743, acc.: 92.97%] [G loss: 3.202440]\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "414 [D loss: 0.127774, acc.: 98.44%] [G loss: 3.719039]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "415 [D loss: 0.178899, acc.: 96.09%] [G loss: 4.157775]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "416 [D loss: 0.152165, acc.: 94.53%] [G loss: 3.855361]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "417 [D loss: 0.176012, acc.: 93.75%] [G loss: 4.231822]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "418 [D loss: 0.287335, acc.: 92.19%] [G loss: 3.061819]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "419 [D loss: 0.101072, acc.: 96.88%] [G loss: 3.093677]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "420 [D loss: 0.092434, acc.: 97.66%] [G loss: 3.718219]\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "421 [D loss: 0.222469, acc.: 92.97%] [G loss: 3.553118]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "422 [D loss: 0.049212, acc.: 99.22%] [G loss: 4.361533]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "423 [D loss: 0.191223, acc.: 94.53%] [G loss: 3.751669]\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "424 [D loss: 0.105877, acc.: 97.66%] [G loss: 3.429641]\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "425 [D loss: 0.068741, acc.: 98.44%] [G loss: 3.690204]\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "426 [D loss: 0.183820, acc.: 93.75%] [G loss: 4.416094]\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "after converting: ['1*2*4*6', '^48.5', '11*0*3*7', '0*2*5*7', '0*3*7', '1*4*6*9', '1*4*6', '4*5*7*9*0', '0*2*4', '^571.75', '0*3*5', '1*2*5', '0*1*2', '11*2*4*5', 'allegro~131.39', '0*1*4*7', 'allegretto~106.99', '0*2*4*5', '0*1*6', '7*9*11', 'animato~118.19', 'F7', '0*2*4*7', 'allegro moderato~128.34', '7*9*11*3', 'largamente~32.6', 'lento~52.5', 'moderate~91.19', '0', 'lento~53.5', 'allegro moderato~130.0', '^1747/6', '10*2*4', 'presto~183.46', '^35/6', 'lento~50.1', '^303.75', '2*6*9', 'maestoso~89.85', 'adagio~57.5', '^71.75', '^16.5', 'vivace~159.49', 'maestoso~87.29', '^91/12', '11*1*3*6', '^98.25', '^5.0', '11*2*5*7', '3*6*8*9', 'maestoso~87.68', 'largamente~33.3', '0*4*6', '^269/12', '0*1*4*7', '^398.5', '^797.5', 'maestoso~88.81', '^3533/12', '5*6*9', '6*8*10*2', '^371.25', '^124.0', '^90.25', '0*3*6*9', '^1051/6', '^3611/12', '10', '1*2*5*9', 'maestoso~88.48', '0*4*5*7', 'maestoso~89.47', '11*0*2', '0*2', '^67/3', '2*5*9', '^539/12', '11*2*5*6', '^297.75', '11*1*5*6', '1*3*5*7', '10*1*2', '3*5*10', '^174.75', '11', '3*5*9*11', '9*10*2*5', '^181.5', '0*3*5*7', '5*6*8*10*1', '10*1*4*6', '5*6*8*0', '12/16', '0*5', '3*4*8*10', '11*1*3*6', '2*3*5*7*10', '0*3*5', '1*4*5', '0']\n"
          ]
        }
      ],
      "source": [
        "gan = GAN(rows=100)    \n",
        "gan.train(epochs=427, batch_size=64, sample_interval=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}